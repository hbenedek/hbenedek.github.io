---
title:  "Bellman Equations and Tabular Methods in Reinforcement Learning"
layout: post
---

UNDER CONSTRUCTION

<style TYPE="text/css">
code.has-jax {font: inherit; font-size: 200%; background: inherit; border: inherit;}
</style>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
         chtml: {
            scale: 1.3
        },
        svg: {
            scale: 1.3
        },
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
    }
});
MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full"></script>

# Markov Decision Process

<p>
Reinforcement Learning problems are most easily formulated as Markov Decision Processes. There is an agent deployed in an environment (for the sake of simplicity we restrict ourselves to discrete time environments) where at every timestep $\huge{t}$ it sits in a state $\huge{s_t} \in S$, and it can choose an action $\huge{a_t}\in\huge{A(s_t)}$ associated with the current state. After completing the action the agent finds itself in another state $\huge{s_{t+1}}$ and receives a reward $\huge{r_t\in\mathbb{R}}$. A problem is said to be episodic, if after a terminal state, the agent restarts its journey from a starting state (consider chess, where every configuration of pieces can be regarded as a state and moving a piece is an action). The process is stochastic because of multiple reasons: After every action, the state, in which the agent lands in is not deterministic, for a given action $\huge{a}$, there can be given transition probabilities $\huge{p^a_{s,s'}}$ corresponding to the probability of the event, that after choosing action $\huge{a}$ from state $\huge{s}$ we end up in state $\huge{s'}$. The rewards can be stochastic, meaning that from the same state, action we are not guaranteed to receive the same amount of reward, it may come from a distribution. The mean reward for a transition is defined by $\huge{R^a_{s,s'}=\mathbb{E}(r|s,a,s')}$. At any point the agent guided by a function $\huge{\pi(s,a)}$, which maps probabilities to every state action pair. The agent chooses its next action according to this policy function. A simple example is the uniform strategy, meaning at every state we randomly pick our next action.
</p>

# Q-values and V-values

The goal of the agent is to maximize the discounted value of the sum of the rewards. We need the discount factor $\huge{\gamma\in[0,1]}$ to ensure that present rewards are more valuable than potential rewards in the far future. Since the MDP is stochastic (it comes from the transition probabilities and both the policy as we will see later) we want to maximize the expectation of the discounted sum of the rewards, that is 

<center>
$\huge{Q(s,a)=\mathbb{E}(r_t + \gamma r_{t+1}+ \gamma^2 r_{t+2}+\ldots|s,a)}$
</center>

this is also called the action-value function, or simply Q-values. The function maps a value to each state-action pair. What we can ask is, what is the value of a specific state, can we derive a simple summary statistic? The V-values aka value function answers this question. The 
total (discounted) expected reward the agent gets starting from state s
$\huge{V(s)=\sum_{a'}\pi(s',a')Q(s',a')}$

We can define policies based on Q-values. 
- The greedy policy is defined as $\huge{a' = argmax_a Q(s',a')}$. So we take the action, which has the highest Q-value for a given state action pair.
- $\huge{\epsilon}$-greedy strategy picks the best action according to the -values with probability $\huge{1-\epsilon}$, but with probability $\huge{\epsilon}$ is picks a random action. The advantage of this policy is taht is enforces exploration of the state space, our agent can discover better rewards and does not stuck at local minimums.

# Bellman Equations 

But can we calculate the Q-values somehow? We can start from the a state and using the expected rewards formula derive a consistency condition, between consecutive states, that the Q-values is must satisfy. 

<center>
$\huge{Q(s,a) = \sum_s' p_{s,s'}\left[r^a_{s,s'}\gamma \sum_{a'} \pi(s',a')Q(s',a') \right]} $
</center>
Similarly, by expanding the definition of the V-values we have the corresponding Bellman equation
<center>
$\huge{V(s) = \sum_{a}\pi(s,a)\sum_{s'}p_{s,s'}\left[r^a_{s,s'}+\gamma V(s')\right]} $
</center>

# SARSA and Eligibility Traces

Usually the state space is huge and it would require lots of computation to determine all Q-values exactly. The SARSA (State-Action-Reward-State-Action) learning algorithm comes to help. The agent plays multiple episodes in the environment and iteratively updates the Q-values based on the rewards it got. Suppose we are at state $\huge{s}$ after taking $\huge{a}$. Now the algorithm proceeds as follows
- Choose an action $\huge{a'}$ according to some policy $\huge{\pi}$.
- Observe $\huge{r}$ and $\huge{s'}$
- Calculate $\huge{\Delta Q(s,a)= \eta [r + \gamma Q (s',a')-Q(s,a)]}$ (TD error) and update the Q-value according to the SARSA update rule $\huge{Q(s,a)=Q(s,a)+\Delta Q(s,a)}$

The disadvantage of SARSA is that it only takes into account the current and the previous state for the update. It means information propagates slowly. For example for an episodic task, where the intermediate rewards are all zeroes, and the final reward is some positive number, even after getting to the terminal state, SARSA would not update the Q-values of the intermediate states, it would just update the Q-value of the last but one state.  As Sutton and Barto wrote: eligibility traces help bridge the gap between events and training information. The method works as follows, at every SARSA update, we update the Q-values for every previously chosen state-action pair, based on the same TD error with a fixed $\huge{\lambda}$ discount factor. We will denote the eligibility table with $\huge{E}$ and the Q-values are stored in $\huge{Q}$

- Initialize $\huge{E}$ as $\huge{0}$ for every state-action pair
- After taking action $\huge{a}$ in state $\huge{s}$, we update the all values by $\huge{E = \lambda \cdot E}$, we update $\huge{E(s,a) = E(s,a) + 1}$ and for every other pair
- Perform a SARSA update for EVERY state-action pair with $\huge{Q = Q + \Delta Q(s,a) E}$

Note that although we update every Q-value, for the state-action pairs, for which the agent did not visit, the corresponding entry of the eligibility table is zero, meaning we did not really update those values.

# Gridworld

Now we will test SARSA and eligibility traces on a simple game called Gridworld. The state space consists of a $\huge{N \times N}$ square lattice, where the square indexed with $\huge{(0,0)}$ is the starting point and $\huge{(N-1,N-1)}$ is the terminal state. At each square the agent can perform an action of moving to an adjacent square. After each move the agent receives a reward.Tthe reward is $\huge{100}$ at the terminal state. And -1 elsewhere. We can make the game more interesting by creating artificial obstacles on the grid. We do this by changing the reward values on the middle of the board to -50 and -20 on the lower left quadrant. We can visualize the rewards on a heatmap with $\huge{N=10}$.

![png](../images/2022-03-08-sarsa/rewards.png)

The task of the agent is to find a route to the lower right corner by iteratively playing the game. If the agent is clever it will figure out to avoid the middle of the board and will go to the right corner first, and down towards the finishing state. Our agent will use eligibility trace with $\huge{\lambda=0.9}$ combined with SARSA algorithm to learn the Q-values with learning rate $\huge{\eta=0.1}$. We set the discount rate $\huge{\gamma=0.9}$ as well. After $\huge{5000}$ episodes we plot the corresponding V-values on a heatmap. The agent will deploy an $\huge{\epsilon}$-greedy strategy with $\huge{\epsilon=0.1}$. We can also observe how the agent improved by plotting the rewards collected in each episode.

![png](../images/2022-03-08-sarsa/vvalues2.png)

Finally we can evaluate the learning by making a test run with a greedy algorithm. The learned path looks the following, indicated with light blue.

![png](../images/2022-03-08-sarsa/optimal.png)
