---
title:  "Bellman Equations and Tabular Methods in Reinforcement Learning"
layout: post
---

UNDER CONSTRUCTION

<style TYPE="text/css">
code.has-jax {font: inherit; font-size: 200%; background: inherit; border: inherit;}
</style>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
         chtml: {
            scale: 1.3
        },
        svg: {
            scale: 1.3
        },
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
    }
});
MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full"></script>

# Problem Formulation
gridworld

# Markov Decision Process

Reinforcement Learning problems are most easily formulated as Markov Decision Processes. There is an agent deployed in an environment (for the sake of simplicity we restrict ourselves to discrete time environments) where at every timestep $\huge{t}$ it sits in a state $\huge{s_t} \in S$, and it can choose an action $\huge{a_t}\huge{A(s_t)}$ associated with the current state. After completing the action the agent finds itself in another state $\huge{s_{t+1}}$ and receives a reward $\huge{r_t\huge{\mathbb{R}}}$. The process is stochastic because of multiple reasons:

- After every action, the state, in which the agent lands in is not deterministic, for a given action $\huge{a}$, there can be given transition probabilities $\huge{p^a_{s,s'}}$ corresponding to the probability of the event, that after choosing action $\huge{a}$ from state $\huge{s}$ we end up in state $\huge{s'}$.
- The rewards can be stochastic, meaning that from the same state, action we are not guaranteed to receive the same amount of reward, it may come from a distribution. The mean reward for a transition is defined by $\huge{R^a_{s,s'}=\mathbb{E}(r|s,a,s')}$.
- At any point the agent guided by a function $\huge{\pi(s,a)}$, which maps probabilities to every state action pair. The agent chooses its next action according to this policy function. A simple example is the uniform strategy, meaning at every state we randomly pick our next action

A problem is said to be episodic, if after a terminal state, the agent restarts its journey from a starting state (consider chess, where every configuration of pieces can be regarded as a state and moving a piece is an action).

# Q-values and V-values

The goal of the agent is to maximize the discounted value of the sum of the rewards. We need the discount factor $\huge{\gamma\in[0,1]}$ to ensure that present rewards are more valuable than potential rewards in the far future. Since the MDP is stochastic (it comes from the transition probabilities and both the policy as we will see later) we want to maximize the expectation of the discounted sum of the rewards, that is 

<center>
$\huge{Q(s,a)=\mathbb{E}(r_t + \gamma r_{t+1}+ \gamma^2 r_{t+2}+\ldots|s,a)}$
</center>

this is also called the action-value function, or simply Q-values. The function maps a value to each state-action pair. What we can ask is, what is the value of a specific state, can we derive a simple summary statistic? The V-values aka value function answers this question. The 
total (discounted) expected reward the agent gets starting from state s
$\huge{V(s)=\sum_{a'}\pi(s',a')Q(s',a')}$

We can define policies based on Q-values. 
- The greedy policy is defined as $\huge{a' = argmax_a Q(s',a')}$. So we take the action, which has the highest Q-value for a given state action pair.
- $\huge{\epsilon}$-greedy strategy picks the best action according to the -values with probability $\huge{1-\epsilon}$, but with probability $\huge{\epsilon}$ is picks a random action. The advantage of this policy is taht is enforces exploration of the state space, our agent can discover better rewards and does not stuck at local minimums.

# Bellman Equations 

But can we calculate the Q-values somehow? We can start from the a state and using the expected rewards formula derive a consistency condition, between consecutive states, that the Q-values is must satisfy.

<center>
$\huge{Q(s,a) = \sum_s' p_{s,s'}\left[r^a_{s,s'}\gamma \sum_{a'} \pi(s',a')Q(s',a') \right]} $
</center>
Similarly, by expanding the definition of the V-values we have the corresponding Bellman equation
<center>
$\huge{V(s) = \sum_{a}\pi(s,a)\sum_{s'}p_{s,s'}\left[r^a_{s,s'}+\gamma V(s')\right]} $
</center>

# SARSA and Eligibility Traces

Usually the state space is huge and it would require lots of computation to determine all Q-values exactly. The SARSA (State-Action-Reward-State-Action) learning algorithm comes to help. The agent plays multiple episodes in the environment and iteratively updates the Q-values based on the rewards it got. Suppose we are at state $\huge{s}$ after taking $\huge{a}$. Now the algorithm proceeds as follows
- Choose an action $\huge{a'}$ according to some policy $\huge{\pi}$.
- Observe $\huge{r}$ and $\huge{s'}$
- Calculate $\huge{\Delta Q(s,a)= \eta [r + \gamma Q (s',a')-Q(s,a)]}$ (TD error) and update the Q-value according to $\huge{Q(s,a)=Q(s,a)+\Delta Q(s,a)}$

The disadvantage of SARSA is that it only takes into account the current and the previous state for the update. It means information propagates slowly. For example for an episodic task, where the intermediate rewards are all zeroes, and the final reward is some positive number, even after getting to the terminal state, SARSA would not update the Q-values of the intermediate states, it would just update the Q-value of the last but one state.  As Sutton and Barto wrote: eligibility traces help bridge the gap between events and training information. The method works as follows, at every SARSA update, we update the Q-values for every previously chosen state-action pair, based on the same TD error with a fixed $\huge{\lambda}$ discount factor. We will denote the eligibility tabel with $\huge{E}$ and the Q-values are stored in $\huge{Q}$

- Initialize $\huge{E}$ as $\huge{0}$ for every state-action pair
- After taking action $\huge{a}$ in state $\huge{s}$, we update the all values by $\huge{E = \lambda \cdot E}$, we update $\huge{E(s,a) = E(s,a) + 1}$ and for every other pair
- Perform a SARSA update for EVERY state-action pair with $\huge{Q = Q + \Delta Q(s,a) E}$

# Calculating the exact Q values

# Learning the Q values